âš¡ Scalable Enterprise Streaming with Spark + Kafka

> ğŸš€ Real-time data pipeline handling **gigabytes to petabytes** of streaming data  
> ğŸ›¡ï¸ Implements **exactly-once delivery semantics** using Kafka, Spark Structured Streaming, and checkpointing  
> ğŸ—ï¸ Designed for **enterprise-grade production use** with modular architecture and fault tolerance

---

## ğŸ“¦ Overview

This project demonstrates a scalable real-time streaming architecture using:
- **Apache Kafka** for ingestion and distributed messaging
- **Apache Spark Structured Streaming** for processing large-scale JSON invoice data
- **Parquet + HDFS/S3/MinIO** for durable, columnar output storage
- **Exactly-once semantics** to avoid data duplication or loss
- Clean logging, modular design, and extensible schema handling

---

## ğŸ§± Architecture

```text
[ Producers (POS / Sensors / APIs) ]
              â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Apache Kafkaâ”‚  â† High-throughput ingest (Partitions = scale)
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Spark Structured Streaming  â”‚
   â”‚ - Schema enforcement        â”‚
   â”‚ - JSON parsing & flattening â”‚
   â”‚ - explode() array data      â”‚
   â”‚ - Exactly-once checkpointingâ”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ Parquet Output Storeâ”‚ â† HDFS / S3 / File_dir
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âœ… Key Features

    âœ… Exactly-once Semantics with Kafka + Spark checkpointing

    âœ… Explodes nested arrays for line-item level granularity

    âœ… Supports large volumes via Kafka partitioning & Spark parallelism

    âœ… Modular, testable code with parameterized paths

    âœ… Schema-first design using StructType for robust ETL

| Component     | Technology                        |
| ------------- | --------------------------------- |
| Messaging     | Apache Kafka                      |
| Processing    | Apache Spark Structured Streaming |
| Storage       | Parquet + HDFS / S3               |
| Serialization | JSON (with schema)                |
| Language      | Python (PySpark)                  |
| Coordination  | Spark Checkpointing (HDFS/Cloud)  |

Invoice-Streaming/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ main.py                # Driver script
â”œâ”€â”€ invoice_processor.py   # Class-based pipeline (read, validate, flatten, write)
â”œâ”€â”€ config/
â”‚   â””â”€â”€ logging.yaml       # Custom logging config
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/               # Landing zone for incoming Kafka batch (simulated here)
â”‚   â””â”€â”€ flattened/         # Output zone (Parquet format)
â””â”€â”€ checkpoints/
    â””â”€â”€ invoice/           # Spark checkpoints (required for exactly-once)

ğŸ›¡ï¸ Exactly-Once Semantics Explained

I'm building this project to ensure exactly-once delivery semantics from Kafka to my Spark Structured Streaming jobs. Here's how I plan to achieve it (and will keep this section updated as I progress):

    âœ… I use Kafka's built-in offset tracking to keep track of processed messages

    âœ… I'm setting a checkpointLocation in Spark (on HDFS, MinIO, or local disk) to store streaming state and enable fault tolerance

    â³ I'm working on using append mode and trigger once/micro-batch options in Spark to ensure consistent writes

    â³ I plan to implement idempotent writes by using deterministic paths and ensuring output directories donâ€™t overwrite partially written files

    ğŸš§ As I scale, Iâ€™ll monitor for duplicates and tune the system to maintain delivery guarantees at large volume

